<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="preload" href="./_next/static/css/d1386df84e9d164b.css" as="style"/><link rel="stylesheet" href="./_next/static/css/d1386df84e9d164b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="./_next/static/chunks/webpack-8e5e5c873be2759d.js" defer=""></script><script src="./_next/static/chunks/framework-052b50cd3d4947f2.js" defer=""></script><script src="./_next/static/chunks/main-98b8f49aadcd70c5.js" defer=""></script><script src="./_next/static/chunks/pages/_app-86112568b43b1067.js" defer=""></script><script src="./_next/static/chunks/3e199aef-89d7c524effe644b.js" defer=""></script><script src="./_next/static/chunks/49-3d197e88747a15b8.js" defer=""></script><script src="./_next/static/chunks/106-a71b11894cb18610.js" defer=""></script><script src="./_next/static/chunks/621-67f971e1eb395bea.js" defer=""></script><script src="./_next/static/chunks/pages/index-dca75ab717967b15.js" defer=""></script><script src="./_next/static/9ZKLgtPWBf9UmQ2Kb2VLH/_buildManifest.js" defer=""></script><script src="./_next/static/9ZKLgtPWBf9UmQ2Kb2VLH/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"result":{"clusters":[{"cluster":"Feedback on Accessibility for Seniors","cluster_id":"2","takeaways":"高齢者向けの資料は、わかりやすさと読みやすさが重要です。参加者は、簡潔な説明や大きな文字が高齢者に適していると評価しました。特に、触りながら学ぶことや、間違えた際のフォローが効果的だと指摘されています。\n\n一方で、80代や90代の高齢者がこのような資料を読むかは疑問視されています。また、特殊詐欺や投資詐欺への注意喚起も重要なテーマとして挙げられました。全体として、70歳以上の人々にとって理解しやすい内容が求められています。","arguments":[{"arg_id":"A1_0","argument":"高齢者には難しい言葉が多くて困った。","comment_id":"1","x":1.2372005,"y":14.783423,"p":1},{"arg_id":"A6_0","argument":"この内容は高齢者にとって基本からわかりやすく、非常に適していると感じました。","comment_id":"6","x":0.87806696,"y":14.056394,"p":1},{"arg_id":"A17_1","argument":"お年寄りは触りながら間違えながら覚えるのが効果的で、間違えた際には戻り方を解説する必要がある。","comment_id":"17","x":1.295957,"y":14.325409,"p":1},{"arg_id":"A18_0","argument":"後期高齢者向けの購入ガイドは、簡潔に説明されており、読みやすい。","comment_id":"18","x":0.59643966,"y":14.087667,"p":1},{"arg_id":"A23_0","argument":"今の70歳代はまだまだ活躍できる。","comment_id":"23","x":0.7971199,"y":15.062084,"p":1},{"arg_id":"A23_2","argument":"高齢者をターゲットにしているように感じる。","comment_id":"23","x":1.4286164,"y":14.461026,"p":1},{"arg_id":"A23_4","argument":"80、90歳代になってもこのような本を読むかは疑問だ。","comment_id":"23","x":0.5390395,"y":14.615307,"p":1},{"arg_id":"A31_1","argument":"特殊詐欺や投資詐欺に注意し、被害者にならないよう努めます。","comment_id":"31","x":1.4828415,"y":14.055905,"p":1},{"arg_id":"A36_0","argument":"70歳女性: まったく役に立たなかった。","comment_id":"36","x":1.0394169,"y":15.185586,"p":1},{"arg_id":"A37_0","argument":"70代の母に贈りました。図書館で一度読んだところ、文字が大きく説明が分かりやすいとのことで、手元に置いておきたいと言われたので購入しました。","comment_id":"37","x":0.057747994,"y":14.381523,"p":1},{"arg_id":"A38_0","argument":"内容は70歳を過ぎた私でも理解できるものでした。","comment_id":"38","x":0.98976433,"y":15.053648,"p":1},{"arg_id":"A41_0","argument":"40歳でも十分に資料性が高い！","comment_id":"41","x":0.7997401,"y":14.721574,"p":1},{"arg_id":"A44_1","argument":"年齢が上がると何が分からないかが分からないので、これぐらいの内容がちょうど良いと思います。","comment_id":"44","x":1.0468484,"y":14.358807,"p":1},{"arg_id":"A49_0","argument":"70歳以上の人々の中には、AIについて理解できない人が多いのではないかと思います。","comment_id":"49","x":1.2839075,"y":14.840948,"p":1}]},{"cluster":"User-Friendliness and Clarity","cluster_id":"1","takeaways":"参加者は、情報の分かりやすさと親しみやすさを評価しています。特に、文字が大きく、写真や易しい文章が復習に役立つとの意見が多く見られました。一方で、初心者には難しい部分もあり、未解決の問題が残っているとの指摘もありました。\n\nAIの活用についても言及があり、エネルギー管理や不確実な状況の理解を助ける点が評価されています。しかし、既に知識を持っている参加者には物足りなさを感じることもあり、内容の充実を求める声もありました。全体として、分かりやすさと情報の深さのバランスが重要であることが浮き彫りになりました。","arguments":[{"arg_id":"A2_0","argument":"The indexing based on use cases and issues is beneficial.","comment_id":"2","x":1.1347442,"y":11.1950035,"p":1},{"arg_id":"A3_0","argument":"この文章はやさしく、丁寧で分かりやすく書かれており、優れています。","comment_id":"3","x":0.31648803,"y":13.1967,"p":1},{"arg_id":"A5_0","argument":"通販では知りたい情報が見つからないことがある。","comment_id":"5","x":-1.7810512,"y":11.340506,"p":1},{"arg_id":"A7_0","argument":"AI technologies can be mastered effectively.","comment_id":"7","x":1.1995481,"y":11.319427,"p":1},{"arg_id":"A8_0","argument":"多くの場合、知りたいことが理解できない。","comment_id":"8","x":-1.5284125,"y":11.666163,"p":1},{"arg_id":"A9_0","argument":"使い方がわからない時に使っています。","comment_id":"9","x":-2.026999,"y":11.760114,"p":1},{"arg_id":"A9_1","argument":"索引が無いので使いにくいです。","comment_id":"9","x":-1.9928869,"y":11.860473,"p":1},{"arg_id":"A10_0","argument":"よかったです","comment_id":"10","x":0.014335776,"y":12.102867,"p":1},{"arg_id":"A11_0","argument":"易しい文章と写真は復習に最適です。","comment_id":"11","x":0.16859646,"y":12.706566,"p":1},{"arg_id":"A14_0","argument":"I expected more surprising applications of AI, but I found it less helpful.","comment_id":"14","x":1.1410992,"y":11.370412,"p":1},{"arg_id":"A15_1","argument":"まったくの初心者には理解が難しいかもしれません。","comment_id":"15","x":-1.6413956,"y":12.473626,"p":1},{"arg_id":"A16_1","argument":"参考になりました。","comment_id":"16","x":-0.23630503,"y":11.800986,"p":1},{"arg_id":"A18_1","argument":"項目ごとに整理されているため、探しやすく、困りごとが分かりやすい。","comment_id":"18","x":0.66698563,"y":12.220591,"p":1},{"arg_id":"A19_0","argument":"優しくて分かり易いので助かりました。","comment_id":"19","x":0.59411466,"y":12.502381,"p":1},{"arg_id":"A20_0","argument":"半分ほどは知っていることでしたが、初めて知ったこともあり、役に立ちました。","comment_id":"20","x":-0.7811208,"y":11.568823,"p":1},{"arg_id":"A21_0","argument":"タイトルは簡単ではない。","comment_id":"21","x":0.14026405,"y":12.815092,"p":1},{"arg_id":"A21_1","argument":"多くの未解決の問題が残っている。","comment_id":"21","x":-1.8174837,"y":11.690668,"p":1},{"arg_id":"A22_0","argument":"文字が大きく、読み手に配慮した親切でわかりやすい内容でした。","comment_id":"22","x":0.45455575,"y":13.390574,"p":1},{"arg_id":"A24_0","argument":"分かりやすい","comment_id":"24","x":0.9048649,"y":12.597564,"p":1},{"arg_id":"A25_2","argument":"メーカーによっては該当項目が隠れていて見つけにくい。","comment_id":"25","x":-1.8158975,"y":11.523084,"p":1},{"arg_id":"A25_3","argument":"一部の機種ではQRコードが読めず、解決方法がLINEの機能に依存することがある。","comment_id":"25","x":-2.7344434,"y":12.329367,"p":1},{"arg_id":"A26_1","argument":"内容はおおよそマスターしていたので、少し物足りなかったです。","comment_id":"26","x":-1.0303693,"y":12.889043,"p":1},{"arg_id":"A26_2","argument":"力がつけば推測できるようになるので、気楽に読める本だと思います。","comment_id":"26","x":0.08321118,"y":13.285263,"p":1},{"arg_id":"A27_0","argument":"AI can optimize energy grids to minimize waste and lower carbon emissions.","comment_id":"27","x":1.4361002,"y":11.078387,"p":0.5431928404031341},{"arg_id":"A29_0","argument":"注文する前から不安でしたが、私の機種について不明な部分がありました。","comment_id":"29","x":-2.4641664,"y":12.464276,"p":1},{"arg_id":"A30_0","argument":"AI can make uncertain situations more certain and easier to understand.","comment_id":"30","x":1.5519935,"y":10.937903,"p":0.3581384370022923},{"arg_id":"A31_2","argument":"これからも読みながら少しずつ覚えてマスターしていきたいです。","comment_id":"31","x":-0.36071432,"y":12.944294,"p":1},{"arg_id":"A32_0","argument":"今読んでいるが、解りやすいです。","comment_id":"32","x":0.5774057,"y":12.89083,"p":1},{"arg_id":"A33_4","argument":"まだ約50%クリアですが、慌てず焦らず、諦めずに少しずつ進んでいます。","comment_id":"33","x":-0.6581537,"y":12.774749,"p":1},{"arg_id":"A34_0","argument":"AIは確認や情報収集のために便利です。","comment_id":"34","x":0.20829009,"y":11.383112,"p":0.8961955730601241},{"arg_id":"A35_2","argument":"この本は本当に必要な情報を分かりやすく教えてくれます。","comment_id":"35","x":-0.2542141,"y":13.689729,"p":1},{"arg_id":"A35_3","argument":"文字が大きいので読みやすいです。","comment_id":"35","x":0.6866641,"y":13.11249,"p":1},{"arg_id":"A36_1","argument":"知っていることしか書いてありませんでした。","comment_id":"36","x":-1.3690538,"y":11.769591,"p":1},{"arg_id":"A39_0","argument":"内容が分かりやすく、理解しやすいです。","comment_id":"39","x":0.80752313,"y":12.688061,"p":1},{"arg_id":"A43_0","argument":"ほとんどの事例はすでに知っていたので、早合点で買ってしまったかもしれません。","comment_id":"43","x":-1.0663625,"y":11.606672,"p":1},{"arg_id":"A44_0","argument":"初歩の知識が得られ満足です。","comment_id":"44","x":-0.6815029,"y":11.850277,"p":1},{"arg_id":"A44_2","argument":"それから先は各自で知識を得れば良いと思います。","comment_id":"44","x":-0.6593276,"y":11.5251255,"p":1},{"arg_id":"A46_0","argument":"万人にちょうどいい内容は難しいが、もう少し充実しても良かった。","comment_id":"46","x":-0.9588507,"y":12.7299,"p":1},{"arg_id":"A46_2","argument":"字が大きく、スペースも広くて読みやすい。","comment_id":"46","x":0.79783976,"y":13.190357,"p":1},{"arg_id":"A46_3","argument":"内容的にはもう一歩進んでも良いかもしれない。","comment_id":"46","x":-0.88809776,"y":12.453239,"p":1},{"arg_id":"A46_6","argument":"続編や上級編があれば良いかもしれない。","comment_id":"46","x":-0.5856655,"y":12.415959,"p":1},{"arg_id":"A48_0","argument":"キャッシュレスについての情報は大体知っています。","comment_id":"48","x":-1.1220624,"y":11.258874,"p":1},{"arg_id":"A48_1","argument":"PayPay以外のキャッシュレスサービスについても知りたいです。","comment_id":"48","x":-1.5306344,"y":11.202074,"p":1},{"arg_id":"A49_1","argument":"返品したい気持ちです。","comment_id":"49","x":-0.062071167,"y":11.952921,"p":1},{"arg_id":"A50_0","argument":"LINEの登録には相手からの認証が必要ですか？","comment_id":"50","x":-2.4856272,"y":11.664054,"p":1},{"arg_id":"A50_1","argument":"迷惑メールの設定について知りたいです。","comment_id":"50","x":-2.3382187,"y":11.991629,"p":1},{"arg_id":"A50_2","argument":"パスワードがわからない時、パスコードが届かないのはなぜですか？","comment_id":"50","x":-2.4375129,"y":11.488755,"p":1}]},{"cluster":"Target Audience Concerns","cluster_id":"0","takeaways":"この意見は、スマートフォンの使い方に関する書籍の内容に対する不満と期待を反映しています。特に、iPhoneに偏った説明や、Androidのバージョンによる画面の違いが高齢者にとって理解を難しくしているとの指摘があります。また、内容が薄く、実用的な情報が少ないと感じる声も多く、初心者向けではなく、ある程度の知識を持つ人向けの内容であることが強調されています。\n\n一方で、スマホを使いこなすことで高齢者の生活が便利になる可能性があるとの意見もあり、特にパソコンに慣れた人には独力で学ぶことが期待されています。全体として、初心者や高齢者向けの具体的でわかりやすいガイドが求められていることが明らかです。","arguments":[{"arg_id":"A4_0","argument":"アンドロイドを使用しているので、参考になるかどうかは微妙です。","comment_id":"4","x":-3.1196504,"y":13.8491745,"p":1},{"arg_id":"A12_0","argument":"アンドロイドとアイフォンの両方を説明することは良い。","comment_id":"12","x":-3.0792148,"y":13.605902,"p":1},{"arg_id":"A13_0","argument":"内容がiPhone使用者に偏っており、Android使用者として不満があった。","comment_id":"13","x":-2.9043386,"y":13.755899,"p":1},{"arg_id":"A15_0","argument":"この内容は、メールやWebを使える程度の人向けです。","comment_id":"15","x":-2.189151,"y":12.866135,"p":1},{"arg_id":"A16_0","argument":"この本は家族に内容を教えるのに役立ちました。","comment_id":"16","x":-0.37127012,"y":14.05812,"p":1},{"arg_id":"A17_0","argument":"アンドロイドとアイフォンの解説はあるが、機種によって画面が異なるためお年寄りには理解が難しい。","comment_id":"17","x":-2.8619714,"y":14.05277,"p":1},{"arg_id":"A23_1","argument":"内容のない本が1,650円（税込み）というのは驚きだ。","comment_id":"23","x":-1.0853782,"y":13.455162,"p":1},{"arg_id":"A23_3","argument":"本の内容は茶飲み話が半分で、実質的に1/4の量しかない。","comment_id":"23","x":-1.0968623,"y":12.991174,"p":1},{"arg_id":"A25_0","argument":"内容は期待できるが、スマホに慣れた人には当たり前のことが多い。","comment_id":"25","x":-2.3542845,"y":13.540188,"p":1},{"arg_id":"A25_1","argument":"Androidスマホではバージョンによって画面が異なることがある。","comment_id":"25","x":-2.8695014,"y":14.072475,"p":1},{"arg_id":"A25_4","argument":"余分な文章が多く、Android機種に対する内容の比較が不十分。","comment_id":"25","x":-2.9580553,"y":13.597183,"p":1},{"arg_id":"A25_5","argument":"初心者向けではなく、少し慣れた人向けの内容である。","comment_id":"25","x":-1.6008755,"y":12.94516,"p":1},{"arg_id":"A26_0","argument":"高齢者がスマホを使いこなせれば便利で頭の運動になると思います。","comment_id":"26","x":-1.5179951,"y":15.1775875,"p":1},{"arg_id":"A28_0","argument":"80歳を過ぎた知り合いのためにスマホを購入しましたが、操作が難しいです。","comment_id":"28","x":-1.6620625,"y":14.902521,"p":1},{"arg_id":"A31_0","argument":"スマホ初心者にとって、説明がわかりやすく勉強になります。","comment_id":"31","x":-1.9190875,"y":14.224881,"p":1},{"arg_id":"A33_0","argument":"私は78歳ですが、初めて本書を使ってスマホの操作を学んでいます。","comment_id":"33","x":-1.2671412,"y":14.882273,"p":1},{"arg_id":"A33_1","argument":"現在、70歳からのスマホのつまづき原因ベスト3を確認中です。","comment_id":"33","x":-1.5721105,"y":15.268917,"p":1},{"arg_id":"A33_2","argument":"本書の手順に従って操作を確認すれば、クリアできると思います。","comment_id":"33","x":-1.2213155,"y":14.117692,"p":1},{"arg_id":"A33_3","argument":"全ての道はホームに通じるので、ホーム画面に戻って操作を進めています。","comment_id":"33","x":-1.6119155,"y":14.126169,"p":1},{"arg_id":"A35_0","argument":"便利な機能が多く書かれた本を読んでも、伝わらなければ自己嫌悪に陥るだけです。","comment_id":"35","x":-1.4488747,"y":13.583465,"p":1},{"arg_id":"A35_1","argument":"生活に必要な機能はそんなに多くは要らないです。","comment_id":"35","x":-1.6863815,"y":13.394793,"p":1},{"arg_id":"A40_0","argument":"スマホの多様な使い方についての本は非常に参考になりました。","comment_id":"40","x":-2.113243,"y":14.491338,"p":1},{"arg_id":"A42_0","argument":"70代の父親にスマホを購入しましたが、一緒に暮らしていないため使い方を教えられません。","comment_id":"42","x":-1.2056881,"y":15.353358,"p":1},{"arg_id":"A42_1","argument":"父親は元々パソコンが好きなので、独力でスマホをマスターしてもらうつもりです。","comment_id":"42","x":-1.2235224,"y":15.030823,"p":1},{"arg_id":"A42_2","argument":"そのため、使い方を学ぶための本を送りました。","comment_id":"42","x":-0.5255932,"y":14.155633,"p":1},{"arg_id":"A45_0","argument":"期待していた操作のやり方が少なく、歳をとった話が多くて残念だった。","comment_id":"45","x":-2.0666466,"y":14.014906,"p":1},{"arg_id":"A46_1","argument":"両親が使い方をよく聞いてくるので、参考になればと思い購入した。","comment_id":"46","x":-0.36930805,"y":14.475973,"p":1},{"arg_id":"A46_4","argument":"アプリのまとめ方は書いてあるが、並べ替えの仕方が書いてないのは残念。","comment_id":"46","x":-2.6585872,"y":13.284275,"p":1},{"arg_id":"A46_5","argument":"ある程度使える人には物足りなさがあると思う。","comment_id":"46","x":-1.7306775,"y":12.77587,"p":1},{"arg_id":"A47_0","argument":"スマホの操作が分からない母のために本を購入しましたが、内容が初心者には難しいと感じました。","comment_id":"47","x":-1.5511222,"y":14.348289,"p":1},{"arg_id":"A47_1","argument":"本は基本的なスマホの操作方法ではなく、特定の状況に対する対処法が中心でした。","comment_id":"47","x":-2.1730735,"y":13.955411,"p":1},{"arg_id":"A47_2","argument":"ある程度のスマホ知識がある人には理解できる内容ですが、初心者には不向きです。","comment_id":"47","x":-2.1112854,"y":13.314303,"p":1}]}],"comments":{"1":{"comment":"やはり、高齢者には難語句が多くて困った、"},"2":{"comment":"使い道、困りごとが索引になっている点が良い。"},"3":{"comment":"やさしく、かつ丁寧に分かりやすく書いてあり優れものです。"},"4":{"comment":"使用しているのが、アンドロイドなので、参考になるような、ならないようなモドカシサ😅"},"5":{"comment":"私が知りたいことは一つも見いだせなかったのです。これが通販のむつかしいところですね。"},"6":{"comment":"基本から解りやすく高齢者にはピッタリと感じました。"},"7":{"comment":"使いこなせる様になった。"},"8":{"comment":"知りたいことが理解できない場合が多い"},"9":{"comment":"使い方がわからない時に使っています。索引が無いので使いにくい。"},"10":{"comment":"よかったです"},"11":{"comment":"易しい文章と写真で復習に最適です。"},"12":{"comment":"アンドロイドとアイホンの双方を説明しているのが良い。"},"13":{"comment":"iPhone使用者に傾いた内容で、Android使用者の私には不満足なところがあった。"},"14":{"comment":"もっと驚くような使い方が有るかと期待したが、あまり私には参考にはならなかった。"},"15":{"comment":"内容は一応メールとWeb程度は出来る人向け。まったくの初心者には猫に小判。"},"16":{"comment":"家の親に内容を教えてあげられる、良い本でした。参考になりました"},"17":{"comment":"アンドロイドとアイフォン別に解説はしてあるが機種によってかなり画面が違って来るのでお年寄りには理解困難。やはり触りながら間違えながら覚えて行くのが頭に入る。間違えたら戻り方を解説するとか…。"},"18":{"comment":"後期高齢者の主人が購入簡素に説明されているので読みやすい項目毎に探し安く困り事が分かり易い"},"19":{"comment":"優しくて分かり易いので助かりました。"},"20":{"comment":"半分ほどはすでに知っていることばかりでしたが、初めて知ったこともあってまあ、役に立ちました。"},"21":{"comment":"タイトル程易しくはない。未解決がたくさん残る。"},"22":{"comment":"文字も大きく、読み手側に立っていて、親切丁寧、わかりやすかったです。"},"23":{"comment":"今の70歳代はまだまだ出来ます。こんな薄っぺらで、内容のない本が1,650円（税込み）とは驚きです。まるで「高齢者を餌にして釣りあげようとしている」は言い過ぎでしょうか。茶飲み話が半分で、アンドロイドとアイフォン版があり実質１/4の量です。そもそも80，90歳代になればこういった本を読むのかどうか分かりませんが。"},"24":{"comment":"分かりやすい"},"25":{"comment":"表題からすると期待できる内容ですが少しスマホ使い慣れた人にとっては当たり前の内容でした。androidスマホではバージョンが異なると書いてある通りの画面委ならない、メーカーによっては全く該当項目が隠れており見つからない、極端の例ではK国の機種では QRコードが読めず、解決方法はLINEの機能で読み取る必要があるなどネットにもいろいろと解決方法が発表されている。余分な文章が多く、android機種に対する記事の内容が比較検討が不十分である。少し慣れた人ならネットで探すなど補えるが、初心者向けではない本である。"},"26":{"comment":"高齢者がスマホを使いこなせたら、とても便利で頭の運動になると思いますが、残念ながら私には書いてあることのおおよそはマスターしていましたので少し物足りなかったか？というところで、星3つです力がつけば推測できたりできるようになりますから、その点では気楽に読める本だと思います"},"27":{"comment":"分かりにくい。"},"28":{"comment":"80歳過ぎた知り合いの為購入しました中々スマホの操作は難しいですね"},"29":{"comment":"注文する前から不安でしたが、やはり私の機種では不明な部分が有りました。"},"30":{"comment":"なんとなくが確実になる、しかもわかりやすい！"},"31":{"comment":"私のようなスマホ初心者にとっては、説明が比較的わかりやすくなっていて大変勉強になります。なお、昨今は特殊詐欺や投資詐欺といった巧妙化した事象が報道されて、注意しなければならない事項などもあり、被害者とならないように努める所存です。これからも時折読みながら、少しずつ覚えてマスターしていきたいと考えている。"},"32":{"comment":"今読んでいるが解借りやすいです"},"33":{"comment":"私は、７８歳になりますが、初めて本書により世界一簡単なスマホの操作と言う事で一つずつ本を見ながら、操作の仕方を確認しています。只今、１２０頁で７０歳からのスマホ「つまづき原因ベスト３」、この本に従って一つ一つ確認していけばクリア－出来ます。全ての道は「ホ－ム」に通じるでホ－ム画面に戻り操作して本書の手順に添って確認して行けば解ってきます。私はまだ約５０％クリア－ですが、慌てず、焦らず、諦めないで時間のある時に少しづつ前に進んでいます。"},"34":{"comment":"知ってていても確認…等のために便利です"},"35":{"comment":"便利な機能をたくさん書いている本を読んでも、伝わらなければ、自己嫌悪に落ちいるだけ。おまけに生活な必要な機能は、そんなにたくさんは要らない。この本は本当に必要な情報を分かりやすく教えてくれます。文字も大きいので読みやすい。"},"36":{"comment":"70歳女性。まったく役にたたなかった。知っている事しか書いてありませんでした。"},"37":{"comment":"７０代の母に贈答しました。図書館で借りてきて一度拝読はしたのですが文字が大きく説明も分かりやすいらしくて手元に置いておきたいとの事でしたので購入しました。"},"38":{"comment":"70歳をすぎた私でも理解できる内容でした。"},"39":{"comment":"内容が分かりやすく、理解しやすいです。"},"40":{"comment":"スマホを色々に使いまくる本で、おおいに参考になりました。"},"41":{"comment":"40歳でも充分過ぎる資料性高い！"},"42":{"comment":"70代でスマホデビューした父親に購入しました。一緒に暮らしていないので、なかなか使い方を教えてあげられないのでこちらの本を送りました。元々パソコンなどは好きなので独力でマスターしてもらおうと思います。"},"43":{"comment":"ただほとんどの事例はすでに知っていたので、ちょっと早合点で買ってしまったかな、と思いました。"},"44":{"comment":"小難しい事は全然ないし、初歩の初歩知識が、得られ満足です。年齢が上がると何が判らないかが判らないのでこれぐらいでちょうど良い。それから先は各々で知識を得れば良いと思います。"},"45":{"comment":"色々な操作のやり方を知りたかったが、歳をとった話しとかが多く期待外れで残念。"},"46":{"comment":"多分、万人にちょうどいい、というのは難しいのだと思いますが、もうちょっと内容があっても良かったかな？よく使い方について両親が聞いてくるので、参考になれば、と購入。字が大きく、スペースも広くて読みやすさはあると思います。ただ、内容的にはもうひと超えあっても良いかも。アプリのまとめ方は書いてあっても、並べ替えの仕方は書いてない、など、ちょっと残念。ある程度使える人なら、物足りなさはあると思います。続編、上級編などあったら良いかしら？"},"47":{"comment":"スマホの操作が分からないと悩む母へ、本の題名につられ購入しました。本の購入時、中身を試し見が出来なかったので、題名とレビューを参考に購入しました。基本的なスマホの操作方法が載っているのかと思ったら、こういう場合はこういう対処をすると言う内容の物が主です。ある程度スマホの知識がある方なら理解出来る内容だと思いますが、初心者がこちらの本を見て理解するには難しいと思います。"},"48":{"comment":"大体、知っている事ばかりでした。キャッシュレスの事を知りたかったのですが、PayPayだけですか?他には無いのでしょうか?"},"49":{"comment":"このようなことが分からない70歳以上の人がどれほどいるのかな? 返品したい気持ち!"},"50":{"comment":"LINEの登録は相手からの認証が必要？迷惑メールの設定について知りたかった、パスワードが解らない時パスコ-ドが届かない。"}},"translations":{"高齢者には難しい言葉が多くて困った。":["高齢者には難しい言葉が多くて困った。"],"The indexing based on use cases and issues is beneficial.":["The indexing based on use cases and issues is beneficial."],"この文章はやさしく、丁寧で分かりやすく書かれており、優れています。":["この文章はやさしく、一貫性で分かりやすく書かれており、優れていると思います。"],"アンドロイドを使用しているので、参考になるかどうかは微妙です。":["アンドロイドを使用しているので、参考になるかどうかは微妙です。"],"通販では知りたい情報が見つからないことがある。":["通訳では知りたい情報が見つからないことがある。"],"この内容は高齢者にとって基本からわかりやすく、非常に適していると感じました。":["この内容は高齢者にとって基本からわかりやすく、非常に適していると感じました。"],"AI technologies can be mastered effectively.":["AI technologies can be mastered effectively."],"多くの場合、知りたいことが理解できない。":["多くの場合、知りたいことが理解できない。"],"使い方がわからない時に使っています。":["使い方がわからない時に使っています。"],"索引が無いので使いにくいです。":["指針が無いので使いにくいです。"],"よかったです":["よかったです"],"易しい文章と写真は復習に最適です。":["易しい文章と写真は復習に最適です。"],"アンドロイドとアイフォンの両方を説明することは良い。":["アンドロイドとアイフォンの両方を説明することは良い。"],"内容がiPhone使用者に偏っており、Android使用者として不満があった。":["内容がiPhone使用者に偏っており、Android使用者として不満があった。"],"I expected more surprising applications of AI, but I found it less helpful.":["私はAIのもっと驚くべき応用を期待していましたが、あまり役に立たないと感じました。"],"この内容は、メールやWebを使える程度の人向けです。":["この内容は、メールやWebを使える程度の人向けです。"],"まったくの初心者には理解が難しいかもしれません。":["まったくの初心者には理解が難しいかもしれません。"],"この本は家族に内容を教えるのに役立ちました。":["この本は家族に内容を教えるのに役立ちました。"],"参考になりました。":["参考になりました。"],"アンドロイドとアイフォンの解説はあるが、機種によって画面が異なるためお年寄りには理解が難しい。":["アンドロイドとアイフォンの解説はあるが、機種によって画面が異なるためお年寄りには理解が難しい。"],"お年寄りは触りながら間違えながら覚えるのが効果的で、間違えた際には戻り方を解説する必要がある。":["お年寄りは触りなから間違えなから覚えるのが効果的で、間違えた時には戻り方を解説する必要がある。"],"後期高齢者向けの購入ガイドは、簡潔に説明されており、読みやすい。":["後期高齢者向けの購入ガイドは、簡潔に説明されており、読みやすい。"],"項目ごとに整理されているため、探しやすく、困りごとが分かりやすい。":["項目ごとに整理されているため、探しやすく、困りごとが分かりやすい。"],"優しくて分かり易いので助かりました。":["優しくて分かりやすいので助かりました。"],"半分ほどは知っていることでしたが、初めて知ったこともあり、役に立ちました。":["半分までは知っていることでしたが、初めて知ったこともあり、役に立ちました。"],"タイトルは簡単ではない。":["タイトルは簡単ではない。"],"多くの未解決の問題が残っている。":["多くの未解決の問題が残っている。"],"文字が大きく、読み手に配慮した親切でわかりやすい内容でした。":["文字が大きく、読み手に配慮した親切でわかりやすい内容でした。"],"今の70歳代はまだまだ活躍できる。":["今の70歳代はまだまだ活動できる。"],"内容のない本が1,650円（税込み）というのは驚きだ。":["内容のない本が1,650円（税抜き）というのは驚きだ。"],"高齢者をターゲットにしているように感じる。":["高齢者をターゲットにしているように感じる。"],"本の内容は茶飲み話が半分で、実質的に1/4の量しかない。":["本の内容は茶飲み話が半分で、実質的に1/4の量しかない。"],"80、90歳代になってもこのような本を読むかは疑問だ。":["80、90歳代になってもこのような本を読むかは疑問だ。"],"分かりやすい":["分かりやすい"],"内容は期待できるが、スマホに慣れた人には当たり前のことが多い。":["内容は期待できるが、スマホに慣れた人には当たり前のことが多い。"],"Androidスマホではバージョンによって画面が異なることがある。":["Androidスマホではバージョンによって画面が異なることがある。"],"メーカーによっては該当項目が隠れていて見つけにくい。":["メーカに対しては適当項目が隠れていて見つけにくい。"],"一部の機種ではQRコードが読めず、解決方法がLINEの機能に依存することがある。":["一部の機種ではQRコードが読めず、解決方法がLINEの機能に依存することがある。"],"余分な文章が多く、Android機種に対する内容の比較が不十分。":["余分な文章が多く、Android機種に対する内容の比較が不十分。"],"初心者向けではなく、少し慣れた人向けの内容である。":["初心者向けではなく、少し慣れた人向けの内容である。"],"高齢者がスマホを使いこなせれば便利で頭の運動になると思います。":["高齢者がスマホを使いこなせれば便利で頭の運動になると思います。"],"内容はおおよそマスターしていたので、少し物足りなかったです。":["内容はおあよそマスターしていたので、少し物足りなかったです。"],"力がつけば推測できるようになるので、気楽に読める本だと思います。":["力がつけば測定できるようになるので、気楽に読める本だと思います。"],"AI can optimize energy grids to minimize waste and lower carbon emissions.":["AIはエネルギーグリッドを最適化して廃棄物を最小限に抑え、炭素排出を減らすことができます。"],"80歳を過ぎた知り合いのためにスマホを購入しましたが、操作が難しいです。":["80歳を過ぎた知り合いのためにスマホを購入しましたが、操作が難しいです。"],"注文する前から不安でしたが、私の機種について不明な部分がありました。":["注文する前から不安でしたが、私の機種について不明な部分がありました。"],"AI can make uncertain situations more certain and easier to understand.":["AIは不確実な状況をより確実にし、理解しやすくすることができます。"],"スマホ初心者にとって、説明がわかりやすく勉強になります。":["スマホ初心者にとって、説明がわかりやすく強化になります。"],"特殊詐欺や投資詐欺に注意し、被害者にならないよう努めます。":["特定の脅威や投資脅威に注意し、被害者にならないよう努めます。"],"これからも読みながら少しずつ覚えてマスターしていきたいです。":["これからも読みながら少しずつ慣れてマスターしていきたいです。"],"今読んでいるが、解りやすいです。":["今読んでいるが、解りやすいです。"],"私は78歳ですが、初めて本書を使ってスマホの操作を学んでいます。":["私は78歳ですが、初めて本を使ってスマホの操作を学んでいます。"],"現在、70歳からのスマホのつまづき原因ベスト3を確認中です。":["現在、70歳からのスマホの使い方原則を確認中です。"],"本書の手順に従って操作を確認すれば、クリアできると思います。":["本書の手順に従って操作を確認すれば、クリアできると思います。"],"全ての道はホームに通じるので、ホーム画面に戻って操作を進めています。":["全ての道はホームに通じるので、ホーム画面に戻って操作を進めています。"],"まだ約50%クリアですが、慌てず焦らず、諦めずに少しずつ進んでいます。":["まだ50%クリアですが、焦らず慌てず、少しずつ進んでいます。"],"AIは確認や情報収集のために便利です。":["AIは確認や情報収集のために便利です。"],"便利な機能が多く書かれた本を読んでも、伝わらなければ自己嫌悪に陥るだけです。":["便利な機能が多く書かれた本を読んでも、従わなければ自己嫌悪に陥るだけです。"],"生活に必要な機能はそんなに多くは要らないです。":["生活に必要な機能はそんなに多くは要らないです。"],"この本は本当に必要な情報を分かりやすく教えてくれます。":["この本は本当に必要な情報を分かりやすく教えてくれます。"],"文字が大きいので読みやすいです。":["文字が大きいので読みやすいです。"],"70歳女性: まったく役に立たなかった。":["70歳女性: まったく役に立たなかった。"],"知っていることしか書いてありませんでした。":["知っていることしか書いてありませんでした。"],"70代の母に贈りました。図書館で一度読んだところ、文字が大きく説明が分かりやすいとのことで、手元に置いておきたいと言われたので購入しました。":["70代の母に贈りました。図書館で一度読んだところ、文字が大きく説明が分かりやすいとのことで、手元に置いておきたいと言われたので購入しました。"],"内容は70歳を過ぎた私でも理解できるものでした。":["内容は70歳を過ぎた私でも理解できるものでした。"],"内容が分かりやすく、理解しやすいです。":["内容が分かりやすく、理解しやすいです。"],"スマホの多様な使い方についての本は非常に参考になりました。":["スマホの多様な使い方についての本は非常に参考になりました。"],"40歳でも十分に資料性が高い！":["40歳でも十分に資質が高い！"],"70代の父親にスマホを購入しましたが、一緒に暮らしていないため使い方を教えられません。":["70代の父親にスマホを購入しましたが、一緒に悩んでいないため使い方を教えられません。"],"父親は元々パソコンが好きなので、独力でスマホをマスターしてもらうつもりです。":["父親は元々パソコンが好きなので、独力でスマホをマスターしようとするつもりです。"],"そのため、使い方を学ぶための本を送りました。":["そのため、使い方を学ぶための本を送りました。"],"ほとんどの事例はすでに知っていたので、早合点で買ってしまったかもしれません。":["まともな事例はすでに知っていたので、早合点で買ってしまったかもしれません。"],"初歩の知識が得られ満足です。":["初歩の知識が得られ満足です。"],"年齢が上がると何が分からないかが分からないので、これぐらいの内容がちょうど良いと思います。":["年齢が上がると何が分からないかが分からないので、これぐらいの内容がちょうど良いと思います。"],"それから先は各自で知識を得れば良いと思います。":["それから先は各自で知識を得れば良いと思います。"],"期待していた操作のやり方が少なく、歳をとった話が多くて残念だった。":["期待していた操作のやり方が少なく、年をとった話が多くて残念だった。"],"万人にちょうどいい内容は難しいが、もう少し充実しても良かった。":["万人にちょうどいい内容は難しいが、もう少し充実しても良かった。"],"両親が使い方をよく聞いてくるので、参考になればと思い購入した。":["両親が使い方をよく聞いてくれるので、参考になればと思い購入した。"],"字が大きく、スペースも広くて読みやすい。":["字が大きく、スパイスも広くて読みやすい。"],"内容的にはもう一歩進んでも良いかもしれない。":["内容的にはもう一段進んでも良かったかもしれない。"],"アプリのまとめ方は書いてあるが、並べ替えの仕方が書いてないのは残念。":["アプリのまとめ方は書いてあるが、並べ替えの仕方が書いてないのは残念。"],"ある程度使える人には物足りなさがあると思う。":["ある程度使える人には物足りなさがあると思う。"],"続編や上級編があれば良いかもしれない。":["続編や上級編があれば良いかもしれない。"],"スマホの操作が分からない母のために本を購入しましたが、内容が初心者には難しいと感じました。":["スマホの操作が分からない母のために本を購入しましたが、内容が初心者には難しいと感じました。"],"本は基本的なスマホの操作方法ではなく、特定の状況に対する対処法が中心でした。":["本は基本的なスマホの操作方法ではなく、特定の状況に対する対処法が中心でした。"],"ある程度のスマホ知識がある人には理解できる内容ですが、初心者には不向きです。":["ある程度のスマホ知識がある人には理解できる内容ですが、初心者には不向きです。"],"キャッシュレスについての情報は大体知っています。":["キャッシュレスについての情報は大体知っています。"],"PayPay以外のキャッシュレスサービスについても知りたいです。":["PayPay以外のキャッシュレスサービスについても知りたいです。"],"70歳以上の人々の中には、AIについて理解できない人が多いのではないかと思います。":["70歳以上の人々の中には、AIについて理解できない人が多いのではないかと思います。"],"返品したい気持ちです。":["返品したい気持ちです。"],"LINEの登録には相手からの認証が必要ですか？":["LINEの登録には相手からの認証が必要ですか？"],"迷惑メールの設定について知りたいです。":["迷惑メールの設定について知りたいです。"],"パスワードがわからない時、パスコードが届かないのはなぜですか？":["パスワードがわからない時、パスコードが届かないのはなぜですか？"],"Feedback on Accessibility for Seniors":["高齢者向けのアクセシビリティに関するフィードバック"],"User-Friendliness and Clarity":["ユーザーフレンドリーさと明確さ"],"Target Audience Concerns":["ターゲットオーディエンスの懸念"],"Argument":["主張"],"Original comment":["元のコメント"],"Representative arguments":["代表的な主張"],"Open full-screen map":["全画面マップを開く"],"Back to report":["レポートに戻る"],"Hide labels":["ラベルを非表示"],"Show labels":["ラベルを表示"],"Show filters":["フィルターを表示"],"Hide filters":["フィルターを非表示"],"Min. votes":["最小投票数"],"Consensus":["合意"],"Showing":["表示中"],"arguments":["引数"],"Reset zoom":["ズームをリセット"],"Click anywhere on the map to close this":["地図の任意の場所をクリックしてこれを閉じる"],"Click on the dot for details":["詳細については点をクリックしてください"],"agree":["同意する"],"disagree":["同意しない"],"Language":["言語"],"English":["英語"],"of total":["合計の"],"Overview":["概要"],"Cluster analysis":["クラスタ分析"],"Representative comments":["代表的なコメント"],"Introduction":["イントロダクション"],"Clusters":["クラスター"],"Appendix":["付録"],"This report was generated using an AI pipeline that consists of the following steps":["このレポートは、以下のステップからなるAIパイプラインを使用して生成されました"],"Step":["ステップ"],"extraction":["抽出"],"show code":["コードを表示"],"hide code":["コードを非表示"],"show prompt":["プロンプトを表示"],"hide prompt":["プロンプトを隠す"],"embedding":["埋め込み"],"clustering":["クラスタリング"],"labelling":["ラベリング"],"takeaways":["要点"],"overview":["概要"],"Japanese":["日本語"],"Smartphone Usage Guide for Seniors":["高齢者のためのスマートフォン使用ガイド"],"What are the main points of feedback from readers about the book '世界一簡単！ 70歳からのスマホの使いこなし術'?":["「世界一簡単！ 70歳からのスマホの使いこなし道」の読者からの主なフィードバックは何ですか？"],"高齢者向けの資料は、わかりやすさと読みやすさが重要です。参加者は、簡潔な説明や大きな文字が高齢者に適していると評価しました。特に、触りながら学ぶことや、間違えた際のフォローが効果的だと指摘されています。\n\n一方で、80代や90代の高齢者がこのような資料を読むかは疑問視されています。また、特殊詐欺や投資詐欺への注意喚起も重要なテーマとして挙げられました。全体として、70歳以上の人々にとって理解しやすい内容が求められています。":["高齢者向けの資料は、わかりやすさと読みやすさが重要です。参加者は、簡潔な説明や大きな文字が高齢者に適していると評価しました。特に、触りながら学ぶことや、間違えた時のフォローが効果的だと指摘されています。\n\n一方で、80代から90代の高齢者がこのような資料を読むかは疑問視されています。また、特定の認知や投資認知への注意喚起も重要なテーマとして提げられました。全体として、70歳以上の人々にとって理解しやすい内容が求められています。"],"参加者は、情報の分かりやすさと親しみやすさを評価しています。特に、文字が大きく、写真や易しい文章が復習に役立つとの意見が多く見られました。一方で、初心者には難しい部分もあり、未解決の問題が残っているとの指摘もありました。\n\nAIの活用についても言及があり、エネルギー管理や不確実な状況の理解を助ける点が評価されています。しかし、既に知識を持っている参加者には物足りなさを感じることもあり、内容の充実を求める声もありました。全体として、分かりやすさと情報の深さのバランスが重要であることが浮き彫りになりました。":["参加者は、情報の分かりやすさと親しみやすさを評価しています。特に、文字が大きく、写真的や易しい文章が復習に役立つとの意見が多く見られました。一方で、初心者には難しい部分もあり、未解決の問題が残っているとの指摘もありました。\n\nAIの活用についても言及があり、エネルギー管理や不確実な状況の理解を助ける点が評価されています。しかし、既に知識を持っている参加者には物足りなさを感じることもあり、内容の充実を求める声もありました。全体として、分かりやすさや情報の深さのバランスが重要であることが浮き彫りになりました。"],"この意見は、スマートフォンの使い方に関する書籍の内容に対する不満と期待を反映しています。特に、iPhoneに偏った説明や、Androidのバージョンによる画面の違いが高齢者にとって理解を難しくしているとの指摘があります。また、内容が薄く、実用的な情報が少ないと感じる声も多く、初心者向けではなく、ある程度の知識を持つ人向けの内容であることが強調されています。\n\n一方で、スマホを使いこなすことで高齢者の生活が便利になる可能性があるとの意見もあり、特にパソコンに慣れた人には独力で学ぶことが期待されています。全体として、初心者や高齢者向けの具体的でわかりやすいガイドが求められていることが明らかです。":["この意見は、スマートフォンの使い方に関する書籍の内容に対する不満と期待を反映しています。特に、iPhoneに偏った説明や、Androidのバージョンによる画面の違いが高齢者にとって理解を難しくしているとの指摘があります。また、内容が薄く、実用的な情報が少ないと感じる声も多く、初心者向けではなく、ある程度の知識を持つ人向けの内容であることが強調されています。\n\n一方で、スマホを使いこなすことが高齢者の生活が便利になる可能性があるとの意見もあり、特にパソコンに慣れた人には独力で学ぶことが期待されている。全体として、初心者や高齢者向けの具体的でわかりやすいガイドが求められていることが明らかです。"],"The public consultation revealed three key clusters regarding accessibility and user-friendliness for seniors. Participants emphasized the need for clear, large-font materials that facilitate learning through hands-on experiences, while also expressing concerns about the readability of such materials for those in their 80s and 90s. Additionally, there were calls for more comprehensive content that balances simplicity with depth, particularly in smartphone usage guides, which currently favor iPhone users and lack practical information for Android devices. Overall, there is a strong demand for tailored, user-friendly resources that cater specifically to the needs of older adults.":["公的な相談は、高齢者のアクセシビリティとユーザーフレンドリーさに関する3つの重要なクラスターを明らかにしました。参加者は、実践的な体験を通じて学習を促進するための明確で大きなフォントの資料の必要性を強調し、80代や90代の人々にとってそのような資料の可読性について懸念を表明しました。さらに、特にスマートフォンの使用ガイドにおいて、シンプルさと深さのバランスを取ったより包括的なコンテンツが求められています。現在、これらのガイドはiPhoneユーザーに有利であり、Androidデバイスに関する実用的な情報が不足しています。全体として、高齢者のニーズに特化した、使いやすいリソースに対する強い需要があります。"],"The following is an analysis of customer reviews for the book '世界一簡単！ 70歳からのスマホの使いこなし術'. This summary is based on 54 reviews collected as of 2025-02-09.":["以下は、書籍「世界一簡単！ 70歳からのスマホの使いこなし道」に関する顧客レビューの分析です。この要約は、2025年2月9日現在で収集された54件のレビューに基づいています。"]},"overview":"The public consultation revealed three key clusters regarding accessibility and user-friendliness for seniors. Participants emphasized the need for clear, large-font materials that facilitate learning through hands-on experiences, while also expressing concerns about the readability of such materials for those in their 80s and 90s. Additionally, there were calls for more comprehensive content that balances simplicity with depth, particularly in smartphone usage guides, which currently favor iPhone users and lack practical information for Android devices. Overall, there is a strong demand for tailored, user-friendly resources that cater specifically to the needs of older adults.","config":{"name":"Smartphone Usage Guide for Seniors","question":"What are the main points of feedback from readers about the book '世界一簡単！ 70歳からのスマホの使いこなし術'?","input":"review","model":"gpt-4o-mini","extraction":{"workers":3,"limit":50,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nYou are a professional research assistant and your job is to help \nme prepare a nice and clean datasets of arguments. \n\nThe context is that we have run a public consultation on the \ntopic of artificial intelligence. I'm going to give you examples \nof arguments that were contributed by the public and I want you \nto help me make them more concise and easy to read. When really \nnecessary, you can also break it down into two separate arguments, \nbut it will often be best to return a single arguments. \n\nPlease return the result as a well-formatted JSON list of strings. \n\n/human\n\nAI technologies should be developed with a focus on reducing their own \nenvironmental impact over their lifecycle.\n\n/ai \n\n[\n  \"We should focus on reducing the environmental impact of AI technologies\"\n]\n\n/human \n\nThere should be a concerted effort to educate the public about the \ncapabilities, limitations, and ethical considerations of AI.\n\n/ai \n\n[\n  \"We should educate the public about the capabilities of AI\",\n  \"We should educate the public about the limitations and ethical considerations of AI\"\n]\n\n/human \n\nAI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\n\n/ ai \n\n[\n  \"AI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\"\n]\n\n/human \n\nAI can help optimize energy grids, reducing waste and carbon emissions.\n\n/ai \n\n[\n  \"AI could optimize energy grids to reduce waste and carbon emissions.\"\n]\n\n","model":"gpt-4o-mini"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"},"translation":{"model":"gpt-4o-mini","languages":["Japanese"],"flags":["JP"],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."},"intro":"The following is an analysis of customer reviews for the book '世界一簡単！ 70歳からのスマホの使いこなし術'. This summary is based on 54 reviews collected as of 2025-02-09.","output_dir":"review","previous":{"name":"Smartphone Usage Guide for Seniors","question":"What are the main points of feedback from readers about the book '世界一簡単！ 70歳からのスマホの使いこなし術'?","input":"review","model":"gpt-4o-mini","extraction":{"workers":3,"limit":100,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nYou are a professional research assistant and your job is to help \nme prepare a nice and clean datasets of arguments. \n\nThe context is that we have run a public consultation on the \ntopic of artificial intelligence. I'm going to give you examples \nof arguments that were contributed by the public and I want you \nto help me make them more concise and easy to read. When really \nnecessary, you can also break it down into two separate arguments, \nbut it will often be best to return a single arguments. \n\nPlease return the result as a well-formatted JSON list of strings. \n\n/human\n\nAI technologies should be developed with a focus on reducing their own \nenvironmental impact over their lifecycle.\n\n/ai \n\n[\n  \"We should focus on reducing the environmental impact of AI technologies\"\n]\n\n/human \n\nThere should be a concerted effort to educate the public about the \ncapabilities, limitations, and ethical considerations of AI.\n\n/ai \n\n[\n  \"We should educate the public about the capabilities of AI\",\n  \"We should educate the public about the limitations and ethical considerations of AI\"\n]\n\n/human \n\nAI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\n\n/ ai \n\n[\n  \"AI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\"\n]\n\n/human \n\nAI can help optimize energy grids, reducing waste and carbon emissions.\n\n/ai \n\n[\n  \"AI could optimize energy grids to reduce waste and carbon emissions.\"\n]\n\n","model":"gpt-4o-mini"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"},"translation":{"model":"gpt-4o-mini","languages":["Japanese"],"flags":["JP"],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."},"intro":"The following is an analysis of customer reviews for the book '世界一簡単！ 70歳からのスマホの使いこなし術'. This summary is based on 54 reviews collected as of 2025-02-09.","output_dir":"review","embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":false,"reason":"nothing changed"},{"step":"embedding","run":false,"reason":"nothing changed"},{"step":"clustering","run":false,"reason":"nothing changed"},{"step":"labelling","run":false,"reason":"nothing changed"},{"step":"takeaways","run":false,"reason":"nothing changed"},{"step":"overview","run":false,"reason":"nothing changed"},{"step":"translation","run":false,"reason":"nothing changed"},{"step":"aggregation","run":false,"reason":"nothing changed"},{"step":"visualization","run":false,"reason":"nothing changed"}],"status":"completed","start_time":"2025-02-09T00:49:12.945775","completed_jobs":[],"lock_until":"2025-02-09T00:54:12.949679","previously_completed_jobs":[{"step":"extraction","completed":"2025-02-09T00:39:56.157139","duration":36.799021,"params":{"workers":3,"limit":100,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nYou are a professional research assistant and your job is to help \nme prepare a nice and clean datasets of arguments. \n\nThe context is that we have run a public consultation on the \ntopic of artificial intelligence. I'm going to give you examples \nof arguments that were contributed by the public and I want you \nto help me make them more concise and easy to read. When really \nnecessary, you can also break it down into two separate arguments, \nbut it will often be best to return a single arguments. \n\nPlease return the result as a well-formatted JSON list of strings. \n\n/human\n\nAI technologies should be developed with a focus on reducing their own \nenvironmental impact over their lifecycle.\n\n/ai \n\n[\n  \"We should focus on reducing the environmental impact of AI technologies\"\n]\n\n/human \n\nThere should be a concerted effort to educate the public about the \ncapabilities, limitations, and ethical considerations of AI.\n\n/ai \n\n[\n  \"We should educate the public about the capabilities of AI\",\n  \"We should educate the public about the limitations and ethical considerations of AI\"\n]\n\n/human \n\nAI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\n\n/ ai \n\n[\n  \"AI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\"\n]\n\n/human \n\nAI can help optimize energy grids, reducing waste and carbon emissions.\n\n/ai \n\n[\n  \"AI could optimize energy grids to reduce waste and carbon emissions.\"\n]\n\n","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-02-09T00:39:57.969814","duration":1.811168,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2025-02-09T00:40:06.415033","duration":8.444333,"params":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2025-02-09T00:40:08.273278","duration":1.857594,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"}},{"step":"takeaways","completed":"2025-02-09T00:40:25.787469","duration":17.510723,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"}},{"step":"overview","completed":"2025-02-09T00:40:28.848799","duration":3.059075,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"}},{"step":"translation","completed":"2025-02-09T00:42:16.276431","duration":107.423562,"params":{"model":"gpt-4o-mini","languages":["Japanese"],"flags":["JP"],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."}},{"step":"aggregation","completed":"2025-02-09T00:42:16.311660","duration":0.031844,"params":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"}},{"step":"visualization","completed":"2025-02-09T00:42:23.516834","duration":7.20354,"params":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"}}],"end_time":"2025-02-09T00:49:12.949660"},"embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"some parameters changed: limit"},{"step":"embedding","run":true,"reason":"some dependent steps will re-run: extraction"},{"step":"clustering","run":true,"reason":"some dependent steps will re-run: embedding"},{"step":"labelling","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"takeaways","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling, takeaways"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: extraction, labelling, takeaways, overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: extraction, clustering, labelling, takeaways, overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"running","start_time":"2025-02-09T13:09:45.469843","completed_jobs":[{"step":"extraction","completed":"2025-02-09T13:10:17.168788","duration":31.69538,"params":{"workers":3,"limit":50,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nYou are a professional research assistant and your job is to help \nme prepare a nice and clean datasets of arguments. \n\nThe context is that we have run a public consultation on the \ntopic of artificial intelligence. I'm going to give you examples \nof arguments that were contributed by the public and I want you \nto help me make them more concise and easy to read. When really \nnecessary, you can also break it down into two separate arguments, \nbut it will often be best to return a single arguments. \n\nPlease return the result as a well-formatted JSON list of strings. \n\n/human\n\nAI technologies should be developed with a focus on reducing their own \nenvironmental impact over their lifecycle.\n\n/ai \n\n[\n  \"We should focus on reducing the environmental impact of AI technologies\"\n]\n\n/human \n\nThere should be a concerted effort to educate the public about the \ncapabilities, limitations, and ethical considerations of AI.\n\n/ai \n\n[\n  \"We should educate the public about the capabilities of AI\",\n  \"We should educate the public about the limitations and ethical considerations of AI\"\n]\n\n/human \n\nAI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\n\n/ ai \n\n[\n  \"AI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\"\n]\n\n/human \n\nAI can help optimize energy grids, reducing waste and carbon emissions.\n\n/ai \n\n[\n  \"AI could optimize energy grids to reduce waste and carbon emissions.\"\n]\n\n","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-02-09T13:10:21.788692","duration":4.618299,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2025-02-09T13:10:31.708968","duration":9.919167,"params":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2025-02-09T13:10:33.441345","duration":1.73174,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"}},{"step":"takeaways","completed":"2025-02-09T13:10:43.065151","duration":9.621606,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"}},{"step":"overview","completed":"2025-02-09T13:10:45.727661","duration":2.658937,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"}},{"step":"translation","completed":"2025-02-09T13:11:52.793903","duration":67.062177,"params":{"model":"gpt-4o-mini","languages":["Japanese"],"flags":["JP"],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."}}],"lock_until":"2025-02-09T13:16:52.796947","current_job":"aggregation","current_job_started":"2025-02-09T13:11:52.796926","translation_prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."}}},"__N_SSG":true},"page":"/","query":{},"buildId":"9ZKLgtPWBf9UmQ2Kb2VLH","assetPrefix":".","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>